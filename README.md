
# ğŸš¨ AlertCircle - Real-Time Bylaw Violation Alert System

AlertCircle is a real-time alerting system that notifies nearby users when bylaw officers are spotted. It simulates user activity, processes geo-tagged alerts, enriches them with user proximity data, and visualizes everything in a live map and notification feed.

![Architecture](./docs/AlertCircle_FinalVersion.jpg)

---

## ğŸ§  Project Highlights

- âš¡ **Real-time stream processing** using Apache Kafka and Spark  
- ğŸŒ **Location-based enrichment** with MongoDB geospatial queries  
- â±ï¸ **Airflow DAG** running every minute to simulate active users  
- ğŸ’¾ **Amazon S3 integration** for archiving enriched alerts  
- ğŸ—ºï¸ **Streamlit dashboard** for live map and alert feed  
- â˜ï¸ **Hybrid architecture** with EC2 and local services  

---

## ğŸ“‚ Project Structure

```
AlertCircle/
â”‚
â”œâ”€â”€ airflow/                  # Airflow DAGs (runs on EC2)
    â””â”€â”€ update_user_locations.py
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ update_user_locations_dag.py
â”‚
â”œâ”€â”€ kafka/                    # Kafka producers and consumers
â”‚   â””â”€â”€ user_report_simulator.py
â”‚   â””â”€â”€ enriched_alerts_to_mongo.py
â”‚
â”œâ”€â”€ spark/                    # Spark structured streaming job
â”‚   â””â”€â”€ process_alerts_stream.py
â”‚
â”œâ”€â”€ streamlit_app/            # Streamlit real-time dashboard
â”‚   â””â”€â”€ streamlit_live_map_app.py
â”‚
â”œâ”€â”€ requirements.txt          
â”œâ”€â”€ README.md                 
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Component Overview

### ğŸ§ User Alert Simulation (Producer)

- Simulated users send alerts every **10 seconds**
- Published to Kafka topic: `raw_alerts`

### ğŸ”¥ Spark Enrichment (Stream Processor)

- Consumes `raw_alerts`
- Enriches alerts with nearby users from MongoDB within 500 meters from the alert point
- Publishes to:
  - Kafka topic: `enriched_alerts`
  - S3 bucket: `enriched-alerts`

### ğŸŒ Airflow DAG (on EC2)

- Runs every **1 minute**
- Simulates active users and locations
- Writes to MongoDB: `latest_user_location` (TTL = 5 minutes)

### ğŸ“¥ Enriched Alert Consumer

- Consumes from `enriched_alerts`
- Writes to MongoDB: `alerts_live` (TTL = 10 minutes)

### ğŸ—ºï¸ Streamlit Dashboard

- Connects to `alerts_live` MongoDB collection
- Shows:
  - Real-time **alert map** for visualize
  - Live **notification feed** for users notifications simulation

![Dashboard](./docs/streamlit_dashboard.png)

---

## ğŸš€ Quickstart

### ğŸ› ï¸ Requirements

- Python 3.10+
- Kafka + Zookeeper running locally
- MongoDB running (locally or on EC2)
- AWS credentials if writing to S3

### ğŸ“¦ Install Dependencies

```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run the System

#### 1. Start the Alert Producer

```bash
python kafka/user_report_simulator.py
```

#### 2. Run Spark Stream Processor

```bash
python spark/process_alerts_stream.py
```

#### 3. Start the Enriched Alert Consumer

```bash
python kafka/enriched_alerts_consumer.py
```

#### 4. Launch the Streamlit Dashboard

```bash
streamlit run streamlit_app/streamlit_live_map_app.py
```

> âœ… Airflow is already running on EC2 and updating user locations every minute.

---

## ğŸ§­ Hybrid Deployment Notes

Due to local hardware limitations, this system uses a hybrid deployment model:

- **EC2 instance** hosts (via Docker Compose):
  - MongoDB
  - Apache Airflow (scheduling DAG)
  - Kafka & Zookeeper
- **Local machine** runs:
  - Spark job for stream processing
  - Kafka producers and consumers
  - Streamlit live dashboard

Due to performance limits on local hardware, this project runs with a hybrid model:

- **EC2** runs MongoDB, Kafka, and Airflow
- **Local machine** runs Spark, producers/consumers, and Streamlit

---

## ğŸ“ˆ Future Enhancements

- [ ] Docker Compose / Kubernetes setup
- [ ] Mobile app or PWA for live alerts
- [ ] User authentication & notification preferences
- [ ] Historical data analytics with AWS Athena

---

## ğŸ“¬ Contact

Maintained by [Nivshitz](https://github.com/Nivshitz)  
For feedback, questions or collaboration â€“ feel free to reach out!

---

ğŸ›¡ï¸ Built with â¤ï¸ as a final project for the Cloud & Big Data Engineering course @ Naya College

---

## ğŸ› ï¸ EC2 Deployment & Developer Notes

### ğŸ” Docker Compose Setup (on EC2)
Kafka, Zookeeper, MongoDB, and Airflow are deployed via Docker Compose on the EC2 instance.

Use the cleaned Docker Compose:
```bash
docker compose up -d
```

### ğŸŒ Airflow Access
Airflow Web UI:  
http://<your-ec2-public-ip>:8082/

### ğŸ” SSH & File Transfer Commands

- **SSH into EC2**
```bash
ssh -i alertcircle-key.pem ec2-user@<EC2-IP>
```

- **Copy file from EC2 to local**
```bash
scp -i alertcircle-key.pem ec2-user@<EC2-IP>:~/alertcircle/docker-compose.yaml .
```

- **Copy file from local to EC2**
```bash
scp -i alertcircle-key.pem <local-file> ec2-user@<EC2-IP>:<remote-path>
```

- **Enter dev_env container**
```bash
docker exec -it dev_env bash
```

### ğŸ” On Every EC2 Reboot
Update Kafkaâ€™s public IP:
```bash
./update-kafka-ip.sh
```

---

